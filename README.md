# Domain Name Generator

This project is a pipeline for generating and evaluating domain names using large language models (LLMs). It leverages the Llama3 model, served via Ollama, for domain name generation and the Gemini API for generating training data and evaluating the quality of the generated domains.

## Project Structure

The project is organized into the following directories and files:

- **`data/`**: Contains all the data used in the project, including the training data, generated domain names, and evaluation results.
- **`src/`**: Contains the Python scripts for the different stages of the pipeline.
- **`notebooks/`**: Contains Jupyter notebooks for model comparison and fine-tuning.
- **`requirements.txt`**: Lists the Python dependencies for the project.

## Pipeline Stages

The pipeline consists of the following stages:

1.  **Data Generation**: The `src/generate_finetune_data.py` script uses the Gemini API to generate a dataset of business descriptions and corresponding domain name suggestions. This data is used to fine-tune the Llama3 model.

2.  **Data Preparation**: The `prepare_data.py` script formats the generated data into the Llama 3 chat template for fine-tuning.

3.  **Inference**: The following scripts are used for inference:
    - `src/generate_domains_base_model.py`: Uses the base Llama3 model to generate domain names.
    - `src/generate_domains_finetuned_model.py`: Uses the fine-tuned Llama3 model to generate domain names.
    - `src/generate_domains_finetuned_model_with_guardrails.py`: An experimental script that adds a validation layer to the fine-tuned model's output to ensure the generated domains are well-formed.

4.  **Evaluation**: The following scripts use the Gemini API as a "judge" to evaluate the quality of the generated domains:
    - `src/evaluate_base_model_with_judge.py`: Evaluates the domains generated by the base Llama3 model.
    - `src/evaluate_finetuned_model_with_judge.py`: Evaluates the domains generated by the fine-tuned Llama3 model.

5.  **Summarization**: The `src/summarize_evaluations.py` script aggregates the evaluation results.

## How to Run

1.  **Install Dependencies**: `pip install -r requirements.txt`
2.  **Set up Environment Variables**: Create a `.env` file and add your `GEMINI_API_KEY`.
3.  **Run the Pipeline**: Execute the scripts in the `src/` directory in the order described above.

## Suggested Improvements

- **Configuration Management**: Centralize all configuration parameters (e.g., model names, file paths) into a single configuration file (e.g., `config.yaml`) to make the project more maintainable and easier to configure.
- **Modularization**: Refactor the code to be more modular. For example, the evaluation logic is duplicated in `src/evaluate_base_model_with_judge.py` and `src/evaluate_finetuned_model_with_judge.py`. This could be extracted into a separate module.
- **Error Handling**: Improve the error handling in the scripts to make them more robust.
- **Testing**: Add unit tests to ensure the correctness of the different components of the pipeline.
- **Experiment Tracking**: Use a tool like MLflow or Weights & Biases to track experiments and log results.
- **Data Augmentation**: Increase the size and diversity of the fine-tuning dataset to improve the model's performance and generalization.
- **Inference Scaling**: Increase the number of business descriptions used for inference to get a more comprehensive evaluation of the model's performance.
